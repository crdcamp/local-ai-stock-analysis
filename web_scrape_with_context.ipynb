{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3bd6be",
   "metadata": {},
   "source": [
    "# Retrieve Article Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f174a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and analyzing webpage...\n",
      "Article data extracted and saved to article_data.json\n",
      "Extraction successful! Data saved to article_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove excessive whitespace, newlines, and special characters\n",
    "    if text:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII for simplicity\n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "def extract_article_data(url):\n",
    "    # Define headers to mimic a browser and reduce blocking\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching and analyzing webpage...\")\n",
    "        # Fetch the webpage with headers\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Initialize context dictionary for the local AI\n",
    "        article_data = {\n",
    "            \"url\": url,\n",
    "            \"title\": \"\",\n",
    "            \"main_content\": [],\n",
    "            \"headings\": [],\n",
    "            \"metadata\": {\n",
    "                \"description\": \"\",\n",
    "                \"keywords\": \"\",\n",
    "                \"author\": \"\",\n",
    "                \"date\": \"\"\n",
    "            },\n",
    "            \"links\": [],\n",
    "            \"images\": []\n",
    "        }\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = soup.find('title') or soup.find('h1')\n",
    "        article_data[\"title\"] = clean_text(title_tag.get_text()) if title_tag else \"No title found\"\n",
    "\n",
    "        # Extract metadata (common meta tags for description, keywords, author, date)\n",
    "        meta_tags = soup.find_all('meta')\n",
    "        for meta in meta_tags:\n",
    "            name = meta.get('name', '').lower()\n",
    "            content = meta.get('content', '')\n",
    "            if name == 'description':\n",
    "                article_data[\"metadata\"][\"description\"] = clean_text(content)\n",
    "            elif name == 'keywords':\n",
    "                article_data[\"metadata\"][\"keywords\"] = clean_text(content)\n",
    "            elif name == 'author':\n",
    "                article_data[\"metadata\"][\"author\"] = clean_text(content)\n",
    "            elif name in ('date', 'publish_date', 'publication_date'):\n",
    "                article_data[\"metadata\"][\"date\"] = clean_text(content)\n",
    "\n",
    "        # Extract headings (h1, h2, h3) for structure and context\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3']):\n",
    "            level = tag.name  # e.g., 'h1', 'h2'\n",
    "            text = clean_text(tag.get_text())\n",
    "            if text:\n",
    "                article_data[\"headings\"].append({\"level\": level, \"text\": text})\n",
    "\n",
    "        # Extract main content: look for common article containers\n",
    "        content_selectors = [\n",
    "            'article',  # HTML5 article tag\n",
    "            'div[class*=\"article\"]', 'div[id*=\"article\"]',\n",
    "            'div[class*=\"content\"]', 'div[id*=\"content\"]',\n",
    "            'div[class*=\"post\"]', 'div[id*=\"post\"]',\n",
    "            'main'  # HTML5 main tag\n",
    "        ]\n",
    "        main_content = []\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                paragraphs = element.find_all(['p', 'div', 'span'])\n",
    "                for p in paragraphs:\n",
    "                    text = clean_text(p.get_text())\n",
    "                    if text and len(text) > 50:  # Filter short, irrelevant text\n",
    "                        main_content.append(text)\n",
    "            if main_content:  # Stop if we found content\n",
    "                break\n",
    "\n",
    "        # Fallback: if no content found, grab all paragraphs\n",
    "        if not main_content:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                text = clean_text(p.get_text())\n",
    "                if text and len(text) > 50:\n",
    "                    main_content.append(text)\n",
    "\n",
    "        article_data[\"main_content\"] = main_content if main_content else [\"No main content found\"]\n",
    "\n",
    "        # Extract links for context\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link_text = clean_text(a_tag.get_text())\n",
    "            link_url = urljoin(url, a_tag['href'])  # Resolve relative URLs\n",
    "            if link_text and link_url:\n",
    "                article_data[\"links\"].append({\"text\": link_text, \"url\": link_url})\n",
    "\n",
    "        # Extract images for context\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            src = urljoin(url, img_tag.get('src', ''))  # Resolve relative URLs\n",
    "            alt = clean_text(img_tag.get('alt', ''))\n",
    "            if src:\n",
    "                article_data[\"images\"].append({\"src\": src, \"alt\": alt})\n",
    "\n",
    "        # Save the structured data as JSON for the local AI\n",
    "        with open(\"article_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(article_data, file, indent=4, ensure_ascii=False)\n",
    "        print(\"Article data extracted and saved to article_data.json\")\n",
    "        \n",
    "        return article_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the page: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://finance.yahoo.com/\"\n",
    "    result = extract_article_data(url)\n",
    "    if result:\n",
    "        print(\"Extraction successful! Data saved to article_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8e0fc",
   "metadata": {},
   "source": [
    "# Analyze Results Using Local LLM\n",
    "\n",
    "This time we'll try using the ```ollama``` python library rather than ```smolagents```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08851d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "\n",
    "def analyze_json():\n",
    "    # Load the JSON file\n",
    "    with open('article_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Prepare a prompt for the model with the JSON data\n",
    "    prompt = f\"\"\"\n",
    "    I have a JSON file containing financial news data from Yahoo Finance. Please analyze the content and provide a general summary of the key points, including:\n",
    "    - Main themes or topics in the news.\n",
    "    - Notable stock ticker changes.\n",
    "    - Any significant events or announcements.\n",
    "    - General sentiment or implications for the market.\n",
    "\n",
    "    Here is the JSON data:\n",
    "    {json.dumps(data, indent=2)}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the prompt to the Ollama model\n",
    "    response: ChatResponse = chat(\n",
    "        model='deepseek-r1:14b',  # Replace with your preferred model if needed\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Print the model's summary\n",
    "    print(\"Summary of the JSON Data:\")\n",
    "    print(response['message']['content'])\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
