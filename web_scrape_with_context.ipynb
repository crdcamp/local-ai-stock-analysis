{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3bd6be",
   "metadata": {},
   "source": [
    "# Retrieve Article Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f174a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and analyzing webpage...\n",
      "Article data extracted and saved to article_data.json\n",
      "Extraction successful! Data saved to article_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove excessive whitespace, newlines, and special characters\n",
    "    if text:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII for simplicity\n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "def extract_article_data(url):\n",
    "    # Define headers to mimic a browser and reduce blocking\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching and analyzing webpage...\")\n",
    "        # Fetch the webpage with headers\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Initialize context dictionary for the local AI\n",
    "        article_data = {\n",
    "            \"url\": url,\n",
    "            \"title\": \"\",\n",
    "            \"main_content\": [],\n",
    "            \"headings\": [],\n",
    "            \"metadata\": {\n",
    "                \"description\": \"\",\n",
    "                \"keywords\": \"\",\n",
    "                \"author\": \"\",\n",
    "                \"date\": \"\"\n",
    "            },\n",
    "            \"links\": [],\n",
    "            \"images\": []\n",
    "        }\n",
    "\n",
    "        # Extract title\n",
    "        title_tag = soup.find('title') or soup.find('h1')\n",
    "        article_data[\"title\"] = clean_text(title_tag.get_text()) if title_tag else \"No title found\"\n",
    "\n",
    "        # Extract metadata (common meta tags for description, keywords, author, date)\n",
    "        meta_tags = soup.find_all('meta')\n",
    "        for meta in meta_tags:\n",
    "            name = meta.get('name', '').lower()\n",
    "            content = meta.get('content', '')\n",
    "            if name == 'description':\n",
    "                article_data[\"metadata\"][\"description\"] = clean_text(content)\n",
    "            elif name == 'keywords':\n",
    "                article_data[\"metadata\"][\"keywords\"] = clean_text(content)\n",
    "            elif name == 'author':\n",
    "                article_data[\"metadata\"][\"author\"] = clean_text(content)\n",
    "            elif name in ('date', 'publish_date', 'publication_date'):\n",
    "                article_data[\"metadata\"][\"date\"] = clean_text(content)\n",
    "\n",
    "        # Extract headings (h1, h2, h3) for structure and context\n",
    "        for tag in soup.find_all(['h1', 'h2', 'h3']):\n",
    "            level = tag.name  # e.g., 'h1', 'h2'\n",
    "            text = clean_text(tag.get_text())\n",
    "            if text:\n",
    "                article_data[\"headings\"].append({\"level\": level, \"text\": text})\n",
    "\n",
    "        # Extract main content: look for common article containers\n",
    "        content_selectors = [\n",
    "            'article',  # HTML5 article tag\n",
    "            'div[class*=\"article\"]', 'div[id*=\"article\"]',\n",
    "            'div[class*=\"content\"]', 'div[id*=\"content\"]',\n",
    "            'div[class*=\"post\"]', 'div[id*=\"post\"]',\n",
    "            'main'  # HTML5 main tag\n",
    "        ]\n",
    "        main_content = []\n",
    "        for selector in content_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                paragraphs = element.find_all(['p', 'div', 'span'])\n",
    "                for p in paragraphs:\n",
    "                    text = clean_text(p.get_text())\n",
    "                    if text and len(text) > 50:  # Filter short, irrelevant text\n",
    "                        main_content.append(text)\n",
    "            if main_content:  # Stop if we found content\n",
    "                break\n",
    "\n",
    "        # Fallback: if no content found, grab all paragraphs\n",
    "        if not main_content:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            for p in paragraphs:\n",
    "                text = clean_text(p.get_text())\n",
    "                if text and len(text) > 50:\n",
    "                    main_content.append(text)\n",
    "\n",
    "        article_data[\"main_content\"] = main_content if main_content else [\"No main content found\"]\n",
    "\n",
    "        # Extract links for context\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            link_text = clean_text(a_tag.get_text())\n",
    "            link_url = urljoin(url, a_tag['href'])  # Resolve relative URLs\n",
    "            if link_text and link_url:\n",
    "                article_data[\"links\"].append({\"text\": link_text, \"url\": link_url})\n",
    "\n",
    "        # Extract images for context\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            src = urljoin(url, img_tag.get('src', ''))  # Resolve relative URLs\n",
    "            alt = clean_text(img_tag.get('alt', ''))\n",
    "            if src:\n",
    "                article_data[\"images\"].append({\"src\": src, \"alt\": alt})\n",
    "\n",
    "        # Save the structured data as JSON for the local AI\n",
    "        with open(\"article_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(article_data, file, indent=4, ensure_ascii=False)\n",
    "        print(\"Article data extracted and saved to article_data.json\")\n",
    "        \n",
    "        return article_data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the page: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://finance.yahoo.com/\"\n",
    "    result = extract_article_data(url)\n",
    "    if result:\n",
    "        print(\"Extraction successful! Data saved to article_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8e0fc",
   "metadata": {},
   "source": [
    "# Analyze Results Using Local LLM\n",
    "\n",
    "This time we'll try using the ```ollama``` python library rather than ```smolagents```\n",
    "\n",
    "Also, we'll only provide in content of the page for the model to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08851d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'headings'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43manalyze_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36manalyze_json\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m     data = json.load(file)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Prepare a focused prompt for the model\u001b[39;00m\n\u001b[32m     11\u001b[39m prompt = \u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43mYou are a financial analyst tasked with summarizing a JSON dataset from Yahoo Finance. Focus on the \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmain_content\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m and \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mheadings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m sections to provide a concise summary of the key financial news and updates. Your summary should include:\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43m- Main themes or topics in the news (e.g., tariffs, stock market, AI, etc.).\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43m- Notable stock ticker changes (e.g., gains, losses) and their context.\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43m- Significant events or announcements (e.g., policy changes, corporate news).\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43m- General sentiment or potential market implications.\u001b[39;49m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[33;43mIgnore speculation about website layout, API usage, or image rendering. Do not focus on duplicates or data structure issues unless they directly impact the financial insights. Keep the summary brief and actionable.\u001b[39;49m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;43mHere is the relevant JSON data:\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[33;43mMain Content: \u001b[39;49m\u001b[38;5;132;43;01m{main_content}\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[33;43mHeadings: \u001b[39;49m\u001b[38;5;132;43;01m{headings}\u001b[39;49;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[33;43m\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmain_content\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Send the prompt to the Ollama model\u001b[39;00m\n\u001b[32m     28\u001b[39m response: ChatResponse = chat(\n\u001b[32m     29\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mdeepseek-r1:14b\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Replace with your preferred model if needed\u001b[39;00m\n\u001b[32m     30\u001b[39m     messages=[\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     ]\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: 'headings'"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "\n",
    "def analyze_json():\n",
    "    # Load the JSON file\n",
    "    with open('article_data.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Prepare a focused prompt for the model\n",
    "    prompt = \"\"\"\n",
    "    You are a financial analyst tasked with summarizing web page content from Yahoo Finance. Your summary should include:\n",
    "    - Main themes or topics in the news (e.g., tariffs, stock market, AI, etc.).\n",
    "    - Notable stock ticker changes (e.g., gains, losses) and their context.\n",
    "    - Significant events or announcements (e.g., policy changes, corporate news).\n",
    "    - General sentiment or potential market implications.\n",
    "    \n",
    "\n",
    "    Here is the relevant JSON data:\n",
    "    Main Content: {main_content}\n",
    "    \"\"\".format(\n",
    "        main_content=json.dumps(data['main_content'], indent=2)\n",
    "    )\n",
    "\n",
    "    # Send the prompt to the Ollama model\n",
    "    response: ChatResponse = chat(\n",
    "        model='deepseek-r1:14b',  # Replace with your preferred model if needed\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Print the model's summary\n",
    "    print(\"Summary of the JSON Data:\")\n",
    "    print(response['message']['content'])\n",
    "\n",
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
